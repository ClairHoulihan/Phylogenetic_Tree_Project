{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# Output dataframe to output.txt file\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     f = open(\"output.txt\", \"a\")\n",
    "#     print(df, file=f)\n",
    "#     f.close()\n",
    "\n",
    "# Print numpy array without trunacation\n",
    "# Default is 1000\n",
    "#np.set_printoptions(threshold=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT\n",
    "# Import data from csv to dataframe\n",
    "# By default, the header is the first row or [0]\n",
    "raw_data = pd.read_csv(\"final_data_1187by114_revised_ForStudents.csv\", \n",
    "                        sep=',')\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "# Remove/drop unnecessary/empty columns (\"static_exports\" and \"section_entry_name\")\n",
    "df = df.dropna(axis=\"columns\", how=\"all\")\n",
    "df = df.drop([\"authentihash\", \"vhash\"], axis=1)\n",
    "# Can also use to drop specific columns: df = df.drop([\"static_exports\", \"section_entry_name\"], axis=1\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "# Error: Showed 1 and 3 empty (\"\") rows in \"authentihash\" and \"vhash,\" respectively...\n",
    "#print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTEN DATA\n",
    "# Flatten columns that have more than one variable:\n",
    "# (\"exports\", \"imports\", \"exe\", \"signature_matches\", \"histogram\",\n",
    "# \"static_imports\", \"url\", \"resource_types\", \"ip_addr\", \"email\")\n",
    "# New dataframe is a placeholder to join the new columns back to original dataframe\n",
    "# temp_df = (df.join(df.pop('exports').str.extractall(\"'(.*?)'\")[0]\n",
    "#                .reset_index(level=1, drop=True)\n",
    "#                .rename('exports'))\n",
    "#            .reset_index(drop=True))\n",
    "\n",
    "# temp_df = df.exports.str.split(';').apply(pd.Series)\n",
    "# temp_df.index = df.set_index([\"exports\", \"imports\", \"exe\", \"signature_matches\", \"histogram\",\n",
    "#                               \"static_imports\", \"url\", \"resource_types\", \"ip_addr\", \"email\"]).index\n",
    "# temp_df.stack().reset_index([\"exports\", \"imports\", \"exe\", \"signature_matches\", \"histogram\",\n",
    "#                               \"static_imports\", \"url\", \"resource_types\", \"ip_addr\", \"email\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING AND ONE-HOT ENCODING\n",
    "# Collect all categorical (non-numeric) data columns using df.select_dtypes()\n",
    "df_categorical = df.select_dtypes(include=[object])\n",
    "\n",
    "# Collect all numeric data columns\n",
    "df_numeric = df.select_dtypes(exclude=[object])\n",
    "\n",
    "# Apply one-hot encoding on categorical data, due to nominal data\n",
    "encode = preprocessing.OneHotEncoder()\n",
    "encode.fit(df_categorical)\n",
    "one_hot_labels = encode.transform(df_categorical).toarray()\n",
    "\n",
    "#print(one_hot_labels[0])\n",
    "\n",
    "# Combine one-hot encoded data with numeric data, to create a 2D array/matrix\n",
    "all_data = np.concatenate((one_hot_labels, df_numeric.values), axis=1)\n",
    "\n",
    "\n",
    "for element in all_data:\n",
    "    for element2 in element:\n",
    "        print(\"%d \" % element2, end='')\n",
    "    print(\"End of element\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e6b4ae182ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#tf.reset_default_graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mindices_for_conversion_to_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=11994, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "x = all_data[:len(all_data)//2]\n",
    "y = all_data[len(all_data)//2:]\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "model.fit(all_data, epochs=150, batch_size=10)\n",
    "\n",
    "accuracy = model.evaluate(all_data)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
